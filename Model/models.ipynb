{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import io\n",
    "import pickle\n",
    "\n",
    "#Load dataset for model v1 (Done by: Jennifer)\n",
    "\"\"\"\n",
    "def load_dataset(database_path):\n",
    "     # Connect to the SQLite database\n",
    "    conn = sqlite3.connect(database_path)\n",
    "\n",
    "    # Query to select all records from the faces table\n",
    "    query = \"SELECT * FROM faces\"\n",
    "\n",
    "    # Fetch records from the database into a Pandas DataFrame\n",
    "    df = pd.read_sql_query(query, conn)\n",
    "    df['image'] = df['image'].apply(lambda x: np.array(pickle.loads(x)))\n",
    "    # Close the database connection\n",
    "    conn.close()\n",
    "    # Convert image bytes to numpy array\n",
    "    num_classes = len(np.unique(df['target']))\n",
    "    return df, num_classes\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#Load dataset for model v2 and v3 (Done by: Sadhana)\n",
    "def load_dataset(train_database_path, val_test_database_path, train_table='faces_train', val_table='faces_val', test_table='faces_test'):\n",
    "     # Connect to the SQLite databases\n",
    "    train_conn = sqlite3.connect(train_database_path)\n",
    "    val_test_conn = sqlite3.connect(val_test_database_path)\n",
    "\n",
    "    # Query to select all records from the augmented dataset for training\n",
    "    train_query = f\"SELECT * FROM {train_table}\"\n",
    "\n",
    "    # Fetch records from the database into a Pandas DataFrame\n",
    "    train_df = pd.read_sql_query(train_query, train_conn)\n",
    "    train_df['image'] = train_df['image'].apply(lambda x: np.array(pickle.loads(x), dtype=np.uint8))\n",
    "\n",
    "    # Query to select all records from the validation dataset\n",
    "    val_query = f\"SELECT * FROM {val_table}\"\n",
    "\n",
    "    # Fetch records from the database into a Pandas DataFrame\n",
    "    val_df = pd.read_sql_query(val_query, val_test_conn)\n",
    "    val_df['image'] = val_df['image'].apply(lambda x: np.array(pickle.loads(x), dtype=np.uint8))\n",
    "\n",
    "     # Query to select all records from the test dataset\n",
    "    test_query = f\"SELECT * FROM {test_table}\" \n",
    "\n",
    "    # Fetch records from the database into a Pandas DataFrame\n",
    "    test_df = pd.read_sql_query(test_query, val_test_conn)\n",
    "    test_df['image'] = test_df['image'].apply(lambda x: np.array(pickle.loads(x), dtype=np.uint8))\n",
    "\n",
    "\n",
    "    # Close the database connections\n",
    "    train_conn.close()\n",
    "    val_test_conn.close()\n",
    "\n",
    "    # Convert image bytes to numpy array\n",
    "    num_classes = len(np.unique(train_df['target']))\n",
    "    return train_df['image'], val_df['image'], test_df['image'], train_df['target'], val_df['target'], test_df['target'], num_classes, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataset for model v1 (Done by: Jennifer)\n",
    "def split_dataset(df, val_size=0.2, test_size=0.2, random_state=0):\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(df['image'].values, df['target'].values, test_size=(val_size + test_size), random_state=random_state)\n",
    "\n",
    "    #Split the temp set into validation and test sets\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=(test_size / (val_size + test_size)), random_state=random_state)\n",
    "\n",
    "    # Print the shapes of the resulting sets\n",
    "    print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "    print(\"Validation set shape:\", X_val.shape, y_val.shape)\n",
    "    print(\"Testing set shape:\", X_test.shape, y_test.shape)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess for model v1\n",
    "def preprocess1(y_train, y_val, y_test, num_classes):\n",
    "    y_train_categorical = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_val_categorical = keras.utils.to_categorical(y_test, num_classes)\n",
    "    y_test_categorical = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    return y_train_categorical, y_val_categorical, y_test_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess for model v2 (Done by: Jennifer)\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from PIL import Image\n",
    "def preprocess2(X_train, X_val, X_test, y_train, y_val, y_test, num_classes):\n",
    "    #One-hot encode the labels\n",
    "    y_train_categorical = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_val_categorical = keras.utils.to_categorical(y_val, num_classes)\n",
    "    y_test_categorical = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    #Resize images to 224x224\n",
    "    X_train = np.array([np.array(Image.fromarray(img).resize((224, 224))) for img in X_train])\n",
    "    X_val = np.array([np.array(Image.fromarray(img).resize((224, 224))) for img in X_val])\n",
    "    X_test = np.array([np.array(Image.fromarray(img).resize((224, 224))) for img in X_test])\n",
    "\n",
    "     #Normalize the images\n",
    "    X_train = preprocess_input(X_train)\n",
    "    X_val = preprocess_input(X_val)\n",
    "    X_test = preprocess_input(X_test)\n",
    "    \n",
    "    plt.imshow(X_train[0])\n",
    "    plt.title('First Image in X_train')\n",
    "    plt.show()\n",
    "\n",
    "    return X_train, X_val, X_test, y_train_categorical, y_val_categorical, y_test_categorical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess for model v3 (Done by: Sadhana)\n",
    "from keras.applications.efficientnet import preprocess_input\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def preprocess3(X_train, X_val, X_test, y_train, y_val, y_test, num_classes):\n",
    "    # One-hot encode the labels\n",
    "    y_train_categorical = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_val_categorical = keras.utils.to_categorical(y_val, num_classes)\n",
    "    y_test_categorical = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    # Resize images to 224x224\n",
    "    X_train_resized = np.array([np.array(Image.fromarray(img).resize((224, 224))) for img in X_train])\n",
    "    X_val_resized = np.array([np.array(Image.fromarray(img).resize((224, 224))) for img in X_val])\n",
    "    X_test_resized = np.array([np.array(Image.fromarray(img).resize((224, 224))) for img in X_test])\n",
    "\n",
    "    # Normalize and preprocess the images\n",
    "    X_train_preprocessed = preprocess_input(X_train_resized)\n",
    "    X_val_preprocessed = preprocess_input(X_val_resized)\n",
    "    X_test_preprocessed = preprocess_input(X_test_resized)\n",
    "\n",
    "    plt.imshow(X_train_preprocessed[0])\n",
    "    plt.title('First Image in X_train')\n",
    "    plt.show()\n",
    "\n",
    "    plt.imshow(X_val_preprocessed[0])\n",
    "    plt.title('First Image in X_val')\n",
    "    plt.show()\n",
    "\n",
    "    plt.imshow(X_test_preprocessed[0])\n",
    "    plt.title('First Image in X_test')\n",
    "    plt.show()\n",
    "\n",
    "    return X_train_preprocessed, X_val_preprocessed, X_test_preprocessed, y_train_categorical, y_val_categorical, y_test_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "import tensorflow as tf\n",
    "#Done by: Jennifer\n",
    "def train_cnn_v1(input_shape, num_classes, X_train, y_train, X_test, y_test):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(2, 2))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(2, 2))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(2, 2))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(2, 2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=10)\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Resizing, Dropout\n",
    "from keras.applications import vgg16\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "#Done by: Jennifer\n",
    "def train_model_v2(num_classes, X_train, y_train, X_val, y_val):\n",
    "   \n",
    "    vgg=vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(224,224,3))\n",
    "    for layer in vgg.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(vgg)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax', kernel_regularizer=regularizers.l1_l2(0.1)))\n",
    "    model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "   \n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=10, callbacks=[early_stopping])\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import EfficientNetV2B0\n",
    "from keras.layers import GlobalAveragePooling2D, Dense, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from keras import regularizers\n",
    "#Done by: Sadhana\n",
    "def train_model_v3(num_classes, X_train, y_train, X_val, y_val):\n",
    "    efficientnet_model = EfficientNetV2B0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    for layer in efficientnet_model.layers[-20:]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(efficientnet_model)\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    # Add BatchNormalization after GlobalAveragePooling2D\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax', kernel_regularizer=regularizers.l1_l2(0.08)))\n",
    "    def schedule(epoch, lr):\n",
    "        if epoch < 4:\n",
    "            return 0.001\n",
    "        else:\n",
    "            return 0.001 * np.exp(0.1 * (4 - epoch))\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(schedule)\n",
    "    model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "   \n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=12, batch_size=15, callbacks=[early_stopping, lr_scheduler])\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score)\n",
    "import numpy as np\n",
    "#Done by: Sadhana\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    # Make predictions on the test set\n",
    "    predicted_probabilities = model.predict(X_test)\n",
    "    predicted_classes = np.argmax(predicted_probabilities, axis=1)\n",
    "\n",
    "    # Check if y_test is one-hot encoded and convert it back to class labels\n",
    "    if len(y_test.shape) > 1 and y_test.shape[1] > 1:\n",
    "        y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy1 = accuracy_score(y_test, predicted_classes)\n",
    "\n",
    "    # Precision\n",
    "    precision = precision_score(y_test, predicted_classes, average='weighted')\n",
    "\n",
    "    # Recall\n",
    "    recall = recall_score(y_test, predicted_classes, average='weighted')\n",
    "\n",
    "    # F1 Score\n",
    "    f1 = f1_score(y_test, predicted_classes, average='weighted')\n",
    "\n",
    "    return accuracy1, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#Done by: Jennifer\n",
    "def visualize_predictions(df, model, X_test, y_test, num_rows=5, num_cols=5, figsize=(20, 20)):\n",
    "    # Make predictions on the test set\n",
    "    predicted_probabilities = model.predict(X_test)\n",
    "    predicted_classes = np.argmax(predicted_probabilities, axis=1)\n",
    "\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    # Loop through the test set and visualize predictions\n",
    "    for i in range(num_rows * num_cols):\n",
    "        \n",
    "        actual_name = df['name'][np.where(df['target'].values == np.argmax(y_test, axis=1)[i])[0][0]]\n",
    "        predicted_name = df['name'][np.where(df['target'].values == predicted_classes[i])[0][0]] \n",
    "\n",
    "        axes[i].imshow(X_test[i])\n",
    "        axes[i].set_title(\"Prediction Class = {}\\nTrue Class = {}\".format(predicted_name, actual_name))\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.5)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "#Load and split data for model v1 \n",
    "\"\"\"\n",
    "df, num_classes = load_dataset('../lfw_augmented_dataset.db')\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(df)\n",
    "\"\"\"\n",
    "\n",
    "#Load split data for model v2 and v3\n",
    "train_database_path = '../Model/Datasets/lfw_augmented_dataset.db'\n",
    "val_test_database_path = '../Model/Datasets/lfw_dataset.db'\n",
    "train_table = 'faces'\n",
    "val_table = 'faces_val'\n",
    "test_table = 'faces_test'\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, num_classes, df = load_dataset(train_database_path, val_test_database_path, train_table, val_table, test_table)\n",
    "\n",
    "#Preprocessing for model 1\n",
    "#y_train, y_val, y_test = preprocess1(y_train, y_val, y_test, num_classes)\n",
    "\n",
    "#Preprocessing for model 2\n",
    "#X_train, X_val, X_test, y_train, y_val, y_test = preprocess2(X_train, X_val, X_test, y_train, y_val, y_test, num_classes)\n",
    "\n",
    "#Preprocessing for model 3\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = preprocess3(X_train, X_val, X_test, y_train, y_val, y_test, num_classes)\n",
    "\n",
    "input_shape = (62, 47, 3)\n",
    "\n",
    "# Train model v1\n",
    "#cnn_model, history = train_cnn_model(input_shape, num_classes, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Train model v2\n",
    "#cnn_model, history = train_model_v2(num_classes, X_train, y_train, X_val, y_val)\n",
    "\n",
    "# Train model v3\n",
    "cnn_model, history = train_model_v3(num_classes, X_train, y_train, X_val, y_val)\n",
    "\n",
    "\n",
    "#Evaluate the model\n",
    "\n",
    "evaluation_results = {}\n",
    "evaluation_results['accuracy'], evaluation_results['precision'], evaluation_results['recall'], evaluation_results['f1'] = evaluate_model(cnn_model, X_test, y_test)\n",
    "\n",
    "# Print or use the results as needed\n",
    "print(\"Accuracy:\", evaluation_results['accuracy'])\n",
    "print(\"Precision:\", evaluation_results['precision'])\n",
    "print(\"Recall:\", evaluation_results['recall'])\n",
    "print(\"F1:\", evaluation_results['f1'])\n",
    "\n",
    "# Save the trained model (Done by: Shahd)\n",
    "save_path = \"./model_registry/\"\n",
    "timestamp = time.strftime(\"%Y%m%d%H%M%S\")\n",
    "cnn_model.save(f'{save_path}model_version_{timestamp}.h5')\n",
    "# when the model is initally trained the way we load the model gives 0.0 for all evaluation metrics\n",
    "# therefore, we save the evaluation metrics in a json file\n",
    "metrics_file_path = os.path.join(save_path, 'evaluation_metrics.json')\n",
    "with open(metrics_file_path, 'w') as metrics_file:\n",
    "    json.dump(evaluation_results, metrics_file)\n",
    "\n",
    "#visualize the model on test set \n",
    "visualize_predictions(df, cnn_model, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot training and validation - accuracy and loss, to check for overfitting\n",
    "#Done by: Jennifer\n",
    "def plot_accuracy_and_loss(hist):\n",
    "    plt.plot(hist.history['accuracy'])\n",
    "    plt.plot(hist.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(hist.history['loss'])\n",
    "    plt.plot(hist.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "plot_accuracy_and_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Visualize feature maps for model v1\n",
    "from numpy import expand_dims\n",
    "from keras.models import Model\n",
    "#Done by: Jennifer\n",
    "def visualize_feature_maps(model, image):\n",
    "    \n",
    "    #Get indices of convolutional layers\n",
    "    conv_layer_indices = [i for i, layer in enumerate(model.layers) if isinstance(layer, Conv2D)]\n",
    "\n",
    "    #Create a list to store convolutional layers\n",
    "    conv_layers = [layer for layer in model.layers if isinstance(layer, Conv2D)]\n",
    "\n",
    "    #Create a new model containing only the convolutional layers\n",
    "    model2 = Model(inputs=model.inputs, outputs=[layer.output for layer in conv_layers])\n",
    "\n",
    "    #Expand the dimensions of the input to match the model's expected input shape\n",
    "    image = expand_dims(image, axis=0)\n",
    "\n",
    "    #Get the feature maps\n",
    "    feature_maps = model2.predict(image)\n",
    "    summed_feature_maps = [fmap_list.sum(axis=-1) for fmap_list in feature_maps]\n",
    "\n",
    "    #Plot the feature maps\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    layer_index = 0\n",
    "\n",
    "    for summed_fmap in summed_feature_maps:\n",
    "        ax = plt.subplot(len(conv_layers), 1, layer_index + 1)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        plt.imshow(summed_fmap[0, :, :], cmap='gray')\n",
    "\n",
    "        layer_index += 1\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "visualize_feature_maps(cnn_model, X_train[0])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "b2b00d4fe3237257791fb3755044766d9808f7f5eb59d1249699542eb434a3d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
